{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1\n",
    "Marissa Wiener <br> 9/23/2016 <br><br>See code and explanations below for problems associated with Project 01.  Problem numbers and answers are denoted in **boldface**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Word Count \n",
    "#### Part A. Characters in Little Women"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First retrieve women.txt (if not already downloaded), which contains the text of *Little Women*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-22 23:14:52--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1053440 (1.0M) [text/plain]\n",
      "Saving to: ‘women.txt.1’\n",
      "\n",
      "women.txt.1         100%[=====================>]   1.00M  --.-KB/s   in 0.07s  \n",
      "\n",
      "2016-09-22 23:14:53 (15.2 MB/s) - ‘women.txt.1’ saved [1053440/1053440]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the number of times where Amy, Beth, Jo, and Meg are mentioned respectively in *Little Women*. The code first separates each word in the text to its own line, transforms all text to lower case, sorts data, and finally finds unique counts for each of the words. **Jo is mentioned 1362 times, Meg is mentioned 686 times, Amy is mentioned 652 times, and Beth is mentioned 467 times.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    WC  Name\n",
      "   ---------\n",
      "    652 amy\n",
      "    467 beth\n",
      "   1362 jo\n",
      "    686 meg\n"
     ]
    }
   ],
   "source": [
    "!echo '    WC  Name'\n",
    "!echo '   ---------'\n",
    "!cat women.txt | grep -oE '\\w{{2,}}'| tr [:upper:] [:lower:] | grep -w 'jo\\|beth\\|meg\\|amy'| sort | uniq -c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. Juliet and Romeo in Romeo and Juliet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, retrieve romeo.txt (if not already downloaded), which contains the text of *Romeo and Juliet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-22 23:15:01--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 178983 (175K) [text/plain]\n",
      "Saving to: ‘romeo.txt.1’\n",
      "\n",
      "romeo.txt.1         100%[=====================>] 174.79K  --.-KB/s   in 0.02s  \n",
      "\n",
      "2016-09-22 23:15:01 (7.90 MB/s) - ‘romeo.txt.1’ saved [178983/178983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to figure out how characters are assigned each line. From the output below, it is shown that for Romeo's lines, \"Rom.\" is used, and similarly, \"Jul.\" is used for Juliet's lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grep: write error\r\n",
      "1424:    They say Jove laughs. O gentle Romeo,\r",
      "\r\n",
      "1490:                       Enter Juliet above.\r",
      "\r\n",
      "1493:  Jul. Three words, dear Romeo, and good night indeed.\r",
      "\r\n",
      "1520:                     Enter Juliet again, [above].\r",
      "\r\n",
      "1523:  Jul. Hist! Romeo, hist! O for a falconer's voice\r",
      "\r\n",
      "1528:    With repetition of my Romeo's name.\r",
      "\r\n",
      "1529:    Romeo!\r",
      "\r\n",
      "1535:  Jul. Romeo!\r",
      "\r\n",
      "1616:                        Enter Romeo.\r",
      "\r\n",
      "1632:    Our Romeo hath not been in bed to-night.\r",
      "\r\n",
      "1714:  Mer. Where the devil should this Romeo be?\r",
      "\r\n",
      "1727:  Ben. Romeo will answer it.\r",
      "\r\n",
      "1734:  Mer. Alas, poor Romeo, he is already dead! stabb'd with a white\r",
      "\r\n",
      "1759:                               Enter Romeo.\r",
      "\r\n",
      "1762:  Ben. Here comes Romeo! here comes Romeo!\r",
      "\r\n",
      "1769:    but not to the purpose. Signior Romeo, bon jour! There's a French\r",
      "\r\n",
      "1829:    art thou sociable, now art thou Romeo; now art thou what thou art, by\r",
      "\r\n",
      "1877:    young Romeo?\r",
      "\r\n",
      "1879:  Rom. I can tell you; but young Romeo will be older when you\r",
      "\r\n",
      "1907:    Romeo, will you come to your father's? We'll to dinner thither.\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -win 'Romeo\\|Juliet' romeo.txt | head -65 | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the output below, **Romeo has 163 lines and Juliet has 117 lines**.  These results were obtained by separating words from the play into their own lines, filtering for cases of \"Rom\" or \"Jul\" since this the syntax for which lines are assigned, sorting output and counting the instances of Juliet and Romeo lines respectively. One thing to note is that the code used to separate each word of the text onto different lines, it removes all punctuation before or after a particular word.  Therefore, when we filter for these words, the \".\" after Rom/Jul is unnecessary to include. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    WC  Name\n",
      "   ---------\n",
      "    117 Jul\n",
      "    163 Rom\n"
     ]
    }
   ],
   "source": [
    "!echo '    WC  Name'\n",
    "!echo '   ---------'\n",
    "!cat romeo.txt | grep -oE '\\w{{2,}}'|grep -w 'Rom\\|Jul'| sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Capital Bikeshare\n",
    "#### Part A. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, retrieve zip file which contains Capital Bikeshare data from 1Q16, unzip it (if this has not been done yet), and rename it to q1.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-22 23:15:52--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10643003 (10M) [application/octet-stream]\n",
      "Saving to: ‘2016q1.csv.zip.1’\n",
      "\n",
      "2016q1.csv.zip.1    100%[=====================>]  10.15M  37.0MB/s   in 0.3s   \n",
      "\n",
      "2016-09-22 23:15:53 (37.0 MB/s) - ‘2016q1.csv.zip.1’ saved [10643003/10643003]\n",
      "\n",
      "Archive:  2016q1.csv.zip\n",
      "  inflating: 2016q1.csv              \n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip'\n",
    "!unzip 2016q1.csv.zip\n",
    "!mv 2016q1.csv q1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the following code to note the column names and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Duration (ms)\r\n",
      "  2: Start date\r\n",
      "  3: End date\r\n",
      "  4: Start station number\r\n",
      "  5: Start station\r\n",
      "  6: End station number\r\n",
      "  7: End station\r\n",
      "  8: Bike number\r\n",
      "  9: Member Type\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n q1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes column 5 of the csv file q1, which is the start station column, sorts the file by starting station, counts the number of starts for each of the stations.  It then sorts in descending order of frequency and prints the 10 most popular departing stations in 1Q16. **The 10 most popular stations are Union Station, Massachusetts Ave & Dupont Station, Lincoln Memorial, Jefferson Dr & 14th St SW, Thomas Circle, 15th & P NW, 14th & V NW, New Hampshire and T St NW, Eastern Market Metro, and 17th and Corcoran stations**.  These stations make sense since they are near popular and highly traveled areas of the city. Personally, I have contributed mostly to the frequency of trips starting from 15th & P St NW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Freq  Station Name\n",
      "   -----------------------------------------------------\n",
      "  13120 Columbus Circle / Union Station\n",
      "   9560 Massachusetts Ave & Dupont Circle NW\n",
      "   9388 Lincoln Memorial\n",
      "   8138 Jefferson Dr & 14th St SW\n",
      "   7479 Thomas Circle\n",
      "   7401 15th & P St NW\n",
      "   6568 14th & V St NW\n",
      "   6491 New Hampshire Ave & T St NW\n",
      "   5649 Eastern Market Metro / Pennsylvania Ave & 7th St SE\n",
      "   5514 17th & Corcoran St NW\n"
     ]
    }
   ],
   "source": [
    "!echo '   Freq  Station Name'\n",
    "!echo '   -----------------------------------------------------'\n",
    "!csvcut -c5 q1.csv | sort | uniq -c | sort -rn | head -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the information above to find the bike number which was most  frequently a departing trip from the most popular departing station, Columbus Circle/Union Station.  It pulls both starting station and bike number from the original csv file, and filters only on trips departing from this station.  From this output, it takes the second column (bike number), sorts data and finds the unique count for each bike number.  The top 12 are displayed below since there was a tie for frequency at 15.  **The bike numbers with the top 10 frequencies were W22227, W21867, W21641, W21538, W21239, W20540, W00714, W22080, W21450, W21076, W00777, and W00288**.  This was suprising and seemed rather low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Freq Bike#\n",
      "    ----------\n",
      "     17 W22227\n",
      "     16 W21867\n",
      "     16 W21641\n",
      "     16 W21538\n",
      "     16 W21239\n",
      "     16 W20540\n",
      "     16 W00714\n",
      "     15 W22080\n",
      "     15 W21450\n",
      "     15 W21076\n",
      "     15 W00777\n",
      "     15 W00288\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!echo '    Freq Bike#'\n",
    "!echo '    ----------'\n",
    "!csvcut -c5,8 q1.csv | grep 'Columbus Circle / Union Station'|csvcut -c2 | sort -n | uniq -c | sort -rn | head -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes column 7 of the csv file q1, which is the end station column, sorts the file by ending station, counts the number of trips ending for each of the stations. It then sorts in descending order of frequency and prints the 10 most popular ending stations in 1Q16. **The 10 most popular stations are Union Station, Massachusetts Ave & Dupont Station, Lincoln Memorial, Jefferson Dr & 14th St SW, 15th & P NW, 14th & V NW, Thomas Circle, New Hampshire and T St NW, 5th & K Street NW, and 17th and Corcoran stations**. These stations are highly correlated to the most popular departing stations, as would be expected considering their locations in the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Freq  Station Name\n",
      "   ------------------------------------------\n",
      "  13880 Columbus Circle / Union Station\n",
      "  11183 Massachusetts Ave & Dupont Circle NW\n",
      "   9419 Lincoln Memorial\n",
      "   8975 Jefferson Dr & 14th St SW\n",
      "   8092 15th & P St NW\n",
      "   7267 14th & V St NW\n",
      "   6997 Thomas Circle\n",
      "   6245 New Hampshire Ave & T St NW\n",
      "   5761 5th & K St NW\n",
      "   5651 17th & Corcoran St NW\n"
     ]
    }
   ],
   "source": [
    "!echo '   Freq  Station Name'\n",
    "!echo '   ------------------------------------------'\n",
    "!csvcut -c7 q1.csv | sort | uniq -c | sort -rn | head -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the information above to find the bike number which was most frequently to end at the most popular destination station, Columbus Circle/Union Station. It pulls both ending station and bike number from the original csv file, and filters only on trips ending from this station. From this output, it takes the second column (bike number), sorts data and finds the unique count for each bike number. The top 15 are displayed below since there was a tie for frequency at 15. **The bike numbers with the top 10 frequencies were W00485, W22227, W22099, W22080, W21239, W21076, W20425, W00714, W21997, W21867, W21641, W21538, W21450, W20540, and W01439.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Freq Bike#\n",
      "    ----------\n",
      "     18 W00485\n",
      "     17 W22227\n",
      "     16 W22099\n",
      "     16 W22080\n",
      "     16 W21239\n",
      "     16 W21076\n",
      "     16 W20425\n",
      "     16 W00714\n",
      "     15 W21997\n",
      "     15 W21867\n",
      "     15 W21641\n",
      "     15 W21538\n",
      "     15 W21450\n",
      "     15 W20540\n",
      "     15 W01439\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!echo '    Freq Bike#'\n",
    "!echo '    ----------'\n",
    "!csvcut -c7,8 q1.csv | grep 'Columbus Circle / Union Station'|csvcut -c2 | sort | uniq -c | sort -rn | head -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I've changed the code above from using \"csvsort\" to \"sort\" because it seems to result in a significantly faster performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following filters can be accessed via the zipfile provided and will be used in the problem below:\n",
    "<br>    1. word-per-line.py \n",
    "<br>    2. lower.py\n",
    "<br>    3. lwr-rm-stp-wds.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code takes the text of *Little Women*, separates words into separate rows, transforms words to lower case, sorts, counts unique words, sorts in descending order of word count frequency, and displays the 10 most popular words. **The 10 most popular words are and, the, to, a, I, of, her, it, in, and you**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WC   Word\n",
      "   ---------\n",
      "   8155 and\n",
      "   7689 the\n",
      "   5152 to\n",
      "   4531 a\n",
      "   4003 i\n",
      "   3523 of\n",
      "   3245 her\n",
      "   2774 it\n",
      "   2503 in\n",
      "   2447 you\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!echo '   WC   Word'\n",
    "!echo '   ---------'\n",
    "!cat women.txt | grep -oE '\\w{{1,}}'|tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first python filter lsted below mimics and replaces the part of the pipe above which separates a word to each line (grep -oE '\\w{{1,}}'). The second converts all of the words to lower case (tr '[:upper:]' '[:lower:]'). The code changes the mode to give everyone execute access to the word-per-line.py and lower.py filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x word-per-line.py\n",
    "!chmod +x lower.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows a preview of what the word-per-line.py filter does. It takes the first two lines of Little Women and separates each word per line.\n",
    "\n",
    "Note: Wei helped me figure out how to remove the punctuation from each line using the \"re\" package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\r\n",
      "Project\r\n",
      "Gutenberg\r\n",
      "EBook\r\n",
      "of\r\n",
      "Little\r\n",
      "Women\r\n",
      "by\r\n",
      "Louisa\r\n",
      "May\r\n",
      "Alcott\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 women.txt | ./word-per-line.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add the next python filter lower.py. This filter takes the list of words generated by word-per-line.py and converts each line to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\r\n",
      "project\r\n",
      "gutenberg\r\n",
      "ebook\r\n",
      "of\r\n",
      "little\r\n",
      "women\r\n",
      "by\r\n",
      "louisa\r\n",
      "may\r\n",
      "alcott\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 women.txt | ./word-per-line.py | ./lower.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As seen below, when juxtaposing the original command to obtain the most common words with the new python filter method, the same result is obtained.** Note that in this example, I included words that were one character long, since otherwise it would exclude the words \"a\" and \"I.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Way:\n",
      "   WC   Word\n",
      "   ---------\n",
      "   8155 and\n",
      "   7689 the\n",
      "   5152 to\n",
      "   4531 a\n",
      "   4003 i\n",
      "   3523 of\n",
      "   3245 her\n",
      "   2774 it\n",
      "   2503 in\n",
      "   2447 you\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "\n",
      "With Python Filters:\n",
      "   WC   Word\n",
      "   ---------\n",
      "   8155 and\n",
      "   7689 the\n",
      "   5152 to\n",
      "   4531 a\n",
      "   4003 i\n",
      "   3523 of\n",
      "   3245 her\n",
      "   2774 it\n",
      "   2503 in\n",
      "   2447 you\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!echo 'Original Way:'\n",
    "!echo '   WC   Word'\n",
    "!echo '   ---------'\n",
    "!cat women.txt | grep -oE '\\w{{1,}}'|tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10\n",
    "!echo '\\nWith Python Filters:'\n",
    "!echo '   WC   Word'\n",
    "!echo '   ---------'\n",
    "!cat women.txt | ./word-per-line.py | ./lower.py | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to easily obtain a full list of stop words, install the stop-words package (ignore this step if package is already installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop-words\n",
      "  Downloading stop-words-2015.2.23.1.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Running setup.py bdist_wheel for stop-words ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/22/74/80/77275c2f9f2f1d9841b51e169a38985640a10fbd2711d10791\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2015.2.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python filter listed below adds another filter after the lower.py filter used before. This last filter imports a list of stop words from the stop-words package, lower-cases them to match the case of words within the file, and then compares these words to the words from *Little Women*. If the word is not in the stop word list, it will be printed.  The code changes the mode to give everyone execute access to the lwr-rm-stp-wds.py filter, and shows what the output looks like for the first 2 lines of *Little Women*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project\r\n",
      "gutenberg\r\n",
      "ebook\r\n",
      "little\r\n",
      "women\r\n",
      "louisa\r\n",
      "may\r\n",
      "alcott\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x lwr-rm-stp-wds.py\n",
    "!head -2 women.txt | ./word-per-line.py | ./lower.py | ./lwr-rm-stp-wds.py  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a list of the top 10 most frequent words which aren't stopwords. **The most popular words are Jo, said, little, one, Meg, Amy, Laurie, like, Don, and will**.  These are mostly character names.  I excluded 1 character words in this filter.  The commented code below the pipe with python filters will result in the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding Stop Words:\n",
      "   WC   Word\n",
      "   ---------\n",
      "   1362 jo\n",
      "    827 said\n",
      "    730 little\n",
      "    725 one\n",
      "    686 meg\n",
      "    652 amy\n",
      "    598 laurie\n",
      "    591 like\n",
      "    550 don\n",
      "    508 will\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!echo 'Excluding Stop Words:'\n",
    "!echo '   WC   Word'\n",
    "!echo '   ---------'\n",
    "!cat women.txt | ./word-per-line.py | ./lower.py |./lwr-rm-stp-wds.py | sort | uniq -c | sort -rn | head -10\n",
    "#!cat women.txt | grep -oE '\\w{{2,}}' | ./lwr-rm-stp-wds.py | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, retrieve zip file which contains data to analyze, unzip all files into a directory named many-texts (if this has not been done yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-22 23:43:09--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12668137 (12M) [application/octet-stream]\n",
      "Saving to: ‘texts.zip’\n",
      "\n",
      "texts.zip           100%[=====================>]  12.08M  33.2MB/s   in 0.4s   \n",
      "\n",
      "2016-09-22 23:43:09 (33.2 MB/s) - ‘texts.zip’ saved [12668137/12668137]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  texts.zip\n",
      "  inflating: many-texts/10001.txt    \n",
      "  inflating: many-texts/10002.txt    \n",
      "  inflating: many-texts/10003.txt    \n",
      "  inflating: many-texts/10004.txt    \n",
      "  inflating: many-texts/10005.txt    \n",
      "  inflating: many-texts/10006.txt    \n",
      "  inflating: many-texts/10007.txt    \n",
      "  inflating: many-texts/10008.txt    \n",
      "  inflating: many-texts/10009.txt    \n",
      "  inflating: many-texts/10010.txt    \n",
      "  inflating: many-texts/10011.txt    \n",
      "  inflating: many-texts/10012.txt    \n",
      "  inflating: many-texts/10013.txt    \n",
      "  inflating: many-texts/10014.txt    \n",
      "  inflating: many-texts/10015.txt    \n",
      "  inflating: many-texts/10016.txt    \n",
      "  inflating: many-texts/10017.txt    \n",
      "  inflating: many-texts/10018.txt    \n",
      "  inflating: many-texts/10019.txt    \n",
      "  inflating: many-texts/10020.txt    \n",
      "  inflating: many-texts/10021.txt    \n",
      "  inflating: many-texts/10023.txt    \n",
      "  inflating: many-texts/10024.txt    \n",
      "  inflating: many-texts/10025.txt    \n",
      "  inflating: many-texts/10026.txt    \n",
      "  inflating: many-texts/10027.txt    \n",
      "  inflating: many-texts/10028.txt    \n",
      "  inflating: many-texts/10029.txt    \n",
      "  inflating: many-texts/10030.txt    \n",
      "  inflating: many-texts/10031.txt    \n",
      "  inflating: many-texts/10032.txt    \n",
      "  inflating: many-texts/10033.txt    \n",
      "  inflating: many-texts/10034.txt    \n",
      "  inflating: many-texts/10035.txt    \n",
      "  inflating: many-texts/10036.txt    \n",
      "  inflating: many-texts/10037.txt    \n",
      "  inflating: many-texts/10038.txt    \n",
      "  inflating: many-texts/10039.txt    \n",
      "  inflating: many-texts/10040.txt    \n",
      "  inflating: many-texts/10041.txt    \n",
      "  inflating: many-texts/10042.txt    \n",
      "  inflating: many-texts/10043.txt    \n",
      "  inflating: many-texts/10045.txt    \n",
      "  inflating: many-texts/10046.txt    \n",
      "  inflating: many-texts/10047.txt    \n",
      "  inflating: many-texts/10048.txt    \n",
      "  inflating: many-texts/10049.txt    \n",
      "  inflating: many-texts/10050.txt    \n",
      "  inflating: many-texts/10051.txt    \n",
      "  inflating: many-texts/10052.txt    \n",
      "  inflating: many-texts/10056.txt    \n",
      "  inflating: many-texts/10059.txt    \n",
      "  inflating: many-texts/10060.txt    \n",
      "  inflating: many-texts/10062.txt    \n",
      "  inflating: many-texts/12370.txt    \n",
      "  inflating: many-texts/12372.txt    \n",
      "  inflating: many-texts/12373.txt    \n",
      "  inflating: many-texts/12374.txt    \n",
      "  inflating: many-texts/12375.txt    \n",
      "  inflating: many-texts/12376.txt    \n",
      "  inflating: many-texts/12377.txt    \n",
      "  inflating: many-texts/12378.txt    \n",
      "  inflating: many-texts/12380.txt    \n",
      "  inflating: many-texts/12381.txt    \n",
      "  inflating: many-texts/12383.txt    \n",
      "  inflating: many-texts/12384.txt    \n",
      "  inflating: many-texts/12385.txt    \n",
      "  inflating: many-texts/12386.txt    \n",
      "  inflating: many-texts/1jcfs10.txt  \n",
      "  inflating: many-texts/2babb10.txt  \n",
      "  inflating: many-texts/3babb10.txt  \n",
      "  inflating: many-texts/50bab10.txt  \n",
      "  inflating: many-texts/ajtl10.txt   \n",
      "  inflating: many-texts/allyr10.txt  \n",
      "  inflating: many-texts/alpsn10.txt  \n",
      "  inflating: many-texts/balen10.txt  \n",
      "  inflating: many-texts/baleng2.txt  \n",
      "  inflating: many-texts/batlf10.txt  \n",
      "  inflating: many-texts/bgopr10.txt  \n",
      "  inflating: many-texts/brnte10.txt  \n",
      "  inflating: many-texts/bstjg10.txt  \n",
      "  inflating: many-texts/cambp10.txt  \n",
      "  inflating: many-texts/canbe10.txt  \n",
      "  inflating: many-texts/cantp10.txt  \n",
      "  inflating: many-texts/cfrz10.txt   \n",
      "  inflating: many-texts/crsnk10.txt  \n",
      "  inflating: many-texts/esbio10.txt  \n",
      "  inflating: many-texts/grybr10.txt  \n",
      "  inflating: many-texts/mklmt10.txt  \n",
      "  inflating: many-texts/morem10.txt  \n",
      "  inflating: many-texts/mspcd10.txt  \n",
      "  inflating: many-texts/penbr10.txt  \n",
      "  inflating: many-texts/pgjr10.txt   \n",
      "  inflating: many-texts/pntvw10.txt  \n",
      "  inflating: many-texts/prcpg10.txt  \n",
      "  inflating: many-texts/prhg10.txt   \n",
      "  inflating: many-texts/prhsb10.txt  \n",
      "  inflating: many-texts/rlsl110.txt  \n",
      "  inflating: many-texts/rlsl210.txt  \n",
      "  inflating: many-texts/rmlav10.txt  \n",
      "  inflating: many-texts/sesli10.txt  \n",
      "  inflating: many-texts/svyrd10.txt  \n",
      "  inflating: many-texts/tecom10.txt  \n",
      "  inflating: many-texts/utrkj10.txt  \n",
      "  inflating: many-texts/vpasm10.txt  \n",
      "  inflating: many-texts/wldsp10.txt  \n",
      "  inflating: many-texts/wtrbs10.txt  \n",
      "  inflating: many-texts/zncli10.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -d many-texts texts.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use GNU parallel to perform the parallelizable steps of the pipe. These steps include separating words to their own line, lower casing, and filtering out stop words. Direct the result of each task to a text file called all-words.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computers / CPU cores / Max jobs to run\n",
      "1:local / 2 / 2\n",
      "\n",
      "Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete\n",
      "ETA: 1s 0left 1.40avg  local:0/108/100%/1.4s \n"
     ]
    }
   ],
   "source": [
    "!ls many-texts/*.txt \\\n",
    "    | parallel --eta -j+0 \"grep -oE '\\w{2,}' {} | tr '[:upper:]' '[:lower:]' | ./lwr-rm-stp-wds.py >> many-texts/all-words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the remaining non-parallelizable tasks to the aggregated file.  First sort, next find unique counts for each word, then sort in reverse numeric order. **The 25 most common words in the files are shown below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19588 one\r\n",
      "  12663 said\r\n",
      "  12338 will\r\n",
      "   9685 man\r\n",
      "   9501 now\r\n",
      "   9121 can\r\n",
      "   8817 like\r\n",
      "   8723 little\r\n",
      "   8607 time\r\n",
      "   8540 upon\r\n",
      "   8361 may\r\n",
      "   7765 gutenberg\r\n",
      "   7490 see\r\n",
      "   7321 well\r\n",
      "   7272 project\r\n",
      "   7014 two\r\n",
      "   6822 great\r\n",
      "   6783 old\r\n",
      "   6750 day\r\n",
      "   6670 work\r\n",
      "   6670 know\r\n",
      "   6623 us\r\n",
      "   6566 good\r\n",
      "   6509 must\r\n",
      "   6444 made\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!sort  many-texts/all-words.txt  \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
